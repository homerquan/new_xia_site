---
title: Teaching American Sign Language in Mixed Reality
authors:
- Qijia Shao
- Amy Sniffen
- Julien Blanchet
- Megan E. Hillis
- Xinyu Shi
- Themistoklis K. Haris
- Jason Liu
- Jason Lamberton
- Melissa Malzkuhn
- Lorna C. Quandt
- James Mahoney
- David J. M. Kraemer
- Xia Zhou
- Devin Balkcom
date: '2020-12-01'
doi: ''
publication_types:
- '2'
publication: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
  Technologies (IMWUT), Volume 4 Issue 4, 2020.
publication_short: IMWUT/UbiComp
award: ''
abstract: This paper presents a holistic system to scale up the teaching and learning
  of vocabulary words of American Sign Language (ASL). The system leverages the most
  recent mixed-reality technology to allow the user to perceive her own hands in an
  immersive learning environment with first- and third-person views for motion demonstration
  and practice. Precise motion sensing is used to record and evaluate motion, providing
  real-time feedback tailored to the specific learner. As part of this evaluation,
  learner motions are matched to features derived from the Hamburg Notation System
  (HNS) developed by sign-language linguists. We develop a prototype to evaluate the
  efficacy of mixed-reality-based interactive motion teaching. Results with 60 participants
  show a statistically significant improvement in learning ASL signs when using our
  system, in comparison to traditional desktop-based, non-interactive learning. We
  expect this approach to ultimately allow teaching and guided practice of thousands
  of signs.
featured: false
nopage: true
image: featured.jpg
image-alt: 3D Wi-Fi reflector brain map
resources:
  - cite.bib
  - ubicomp20-motion.pdf
cite: "cite.bib"
pdf: ubicomp20-motion.pdf
---


