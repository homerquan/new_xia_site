[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Xia Zhou",
    "section": "",
    "text": "Xia Zhou\nAssociate Professor\nDepartment of Computer Science, Columbia University\n\n\nI am an Associate Professor in the Department of Computer Science at Columbia University. I direct the Mobile X Lab. Before joining Columbia, I was the co-director of the Dartmouth Networking and Ubiquitous Systems (DartNets) Lab and the Dartmouth Reality and Robotics Lab (RLab) at Dartmouth College. I was also affiliated with the Augmented Health Lab. I received my PhD in Computer Science at UC Santa Barbara in June 2013, working under the supervision of Prof. Heather Zheng. I was a visiting faculty in National Taiwan University from December 2016 to February 2017, and in University of Cambridge from April 2017 to June 2017.\nMy research interest lies broadly in mobile computing and its intersection with other disciplines. Most of my current projects center on light—a ubiquitous medium around us. We explore a range of projects that turn light into a powerful medium for data communication and object/behavioral sensing. Check out our demo videos to learn more details. My PhD work designed network systems to handle the massive volume and unpredictable nature of today’s data traffic (see more in our MobiCom’08\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisual tags (e.g.,barcodes, QR codes) are ubiquitous in modern day life, though they rely on obtrusive geometric patterns to encode data, degrading the overall user experience. We propose a new paradigm of passive visual tags which utilizes light polarization to imperceptibly encode data using cheap, widely-available components. The tag and its data can be extracted from background scenery using off-the-shelf cameras with inexpensive LCD shutters attached atop camera lenses. We examine the feasibility of this design with real-world experiments. Initial results show zero bit errors at distances up to 3.0~m, an angular-detection range of 110’’, and robustness to manifold ambient light and occlusion scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.\n\n\n\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, presents the first demonstration of wireless, 3D localization across the air-water interface – eliminating the need for additional infrastructure on the water surface. Specifically, we propose a laser-based sensing system to enable aerial drones to directly locate underwater robots. The Sunflower system consists of a queen and a worker component on a drone and each tracked underwater robot, respectively. To achieve robust sensing, key system elements include (1) a pinhole- based sensing mechanism to address the sensing skew at air-water boundary and determine the incident angle on the worker, (2) a novel optical-fiber sensing ring to sense weak retroreflected light, (3) a laser-optimized backscatter communication design that exploits laser polarization to maximize retroreflected energy, and (4) the necessary models and algorithms for underwater sensing. Real- world experiments demonstrate that our Sunflower system achieves average localization error of 9.7 cm with ranges up to 3.8 m and is robust against ambient light interference and wave conditions.\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ability to communicate and sense across the air- water boundary is essential for efficient exploration and monitoring of the underwater world. Existing wireless solutions for communication and sensing typically focus on a single physical medium and fall short in achieving high-bandwidth communication and accurate sensing across the air-water interface without any relays on the water surface. We study the use of laser light in this context given its ability to effectively pass the air-water boundary. We present a holistic system framework to address practical challenges such as ambient light interference and environmental dynamics. The proposed AmphiLight framework achieves 5 Mbps bi-directional throughput and zero bit error rate with ranges up to 6.1 m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics. Our ongoing effort extends to realizing robust air-water sensing that enables aerial drones to track multiple underwater robots for topology planning and coordination.\n\n\n\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFace touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth)increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSenseintegrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch.Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model\n\n\n\n\n\nSep 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis paper presents a holistic system to scale up the teaching and learning of vocabulary words of American Sign Language (ASL). The system leverages the most recent mixed-reality technology to allow the user to perceive her own hands in an immersive learning environment with first- and third-person views for motion demonstration and practice. Precise motion sensing is used to record and evaluate motion, providing real-time feedback tailored to the specific learner. As part of this evaluation, learner motions are matched to features derived from the Hamburg Notation System (HNS) developed by sign-language linguists. We develop a prototype to evaluate the efficacy of mixed-reality-based interactive motion teaching. Results with 60 participants show a statistically significant improvement in learning ASL signs when using our system, in comparison to traditional desktop-based, non-interactive learning. We expect this approach to ultimately allow teaching and guided practice of thousands of signs.\n\n\n\n\n\nDec 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe propose a compact noninvasive glucose monitoring system using polarized light, where a user simply needs to place her palm on the device for measuring her current glucose concentration level. The primary innovation of our system is the ability to minimize light scattering from the skin and extract weak changes in light polarization to estimate glucose concentration, all using low-cost hardware. Our system exploits multiple wavelengths and light intensity levels to mitigate the effect of user diversity and confounding factors (e.g., collagen and elastin in the dermis). It then infers glucose concentration using a generic learning model, thus no additional calibration is needed. We design and fabricate a compact (17 cm × 10 cm × 5 cm) and low-cost (i.e., &lt;$250) prototype using off-the-shelf hardware. We evaluate our system with 41 diabetic patients and 9 healthy participants. In comparison to a continuous glucose monitor approved by U.S. Food and Drug Administration (FDA), 89% of our results are within zone A (clinically accurate) of the Clarke Error Grid. The absolute relative difference (ARD) is 10% . The r and p values of the Pearson correlation coefficients between our predicted glucose concentration and reference glucose concentration are 0.91 and 1.6 × 10−143, respectively. These errors are comparable with FDA-approved glucose sensors, which achieve ≈90% clinical accuracy with a 10% mean ARD.\n\n\n\n\n\nNov 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAir-water communication is fundamental for efficient underwater operations, such as environmental monitoring, surveying, or coordinating of heterogeneous aerial and underwater systems. Existing wireless techniques mostly focus on a single physical medium and fall short in achieving high-bandwidth bidirectional communication across the air-water interface. We propose a bidirectional, direct air-water wireless communication link based on laser light, capable of (1) adapting to water dynamics with ultrasonic sensing and (2) steering within a full 3D hemisphere using only a MEMS mirror and passive optical elements. In real-world experiments, our system achieves static throughputs up to 5.04 Mbps, zero-BER transmission ranges up to 6.1m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics.\n\n\n\n\n\nNov 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this paper we describe and characterize the largest Wi-Fi network trace ever published: spanning seven years, approximately 3000 distinct access points, 40,000 authenticated users, and 600,000 distinct Wi-Fi stations. The 7TB of raw data are pre-processed into connection sessions, which are made available for the research community. We describe the methods used to capture and process the traces, and characterize the most prominent trends and changes during the seven-year span of the trace. Furthermore, this Wi-Fi network covers the campus of Dartmouth College, the same campus detailed a decade earlier in seminal papers about that network and its users’ network behavior. We thus are able to comment on changes in patterns of usage, connection, and mobility in Wi-Fi deployments.\n\n\n\n\n\nNov 1, 2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#xia-zhou",
    "href": "index.html#xia-zhou",
    "title": "Xia Zhou",
    "section": "",
    "text": "Associate Professor\nDepartment of Computer Science, Columbia University"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Xia Zhou",
    "section": "About",
    "text": "About\nI am an Associate Professor in the Department of Computer Science at Columbia University. I direct the Mobile X Lab. Before joining Columbia, I was the co-director of the Dartmouth Networking and Ubiquitous Systems (DartNets) Lab and the Dartmouth Reality and Robotics Lab (RLab) at Dartmouth College.\n…\n(Continue with all your content)"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Xia Zhou",
    "section": "Publications",
    "text": "Publications\nSee my publications on Google Scholar."
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Xia Zhou",
    "section": "Teaching",
    "text": "Teaching\n\nCOMS W4118 Operating Systems – Fall 2019\n\nCOMS W3157 Advanced Programming – Spring 2019\n\nCOMS W4115 Programming Languages and Translators – Fall 2018\n\nCOMS W1007 Honors Introduction to Computer Science – Spring 2018"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Xia Zhou",
    "section": "CV",
    "text": "CV\nDownload CV (PDF)"
  },
  {
    "objectID": "index.html#mobile-x-lab",
    "href": "index.html#mobile-x-lab",
    "title": "Xia Zhou",
    "section": "Mobile X Lab",
    "text": "Mobile X Lab\nVisit the Mobile X Lab to learn more about our ongoing research and team."
  },
  {
    "objectID": "index.html#top-publications",
    "href": "index.html#top-publications",
    "title": "Xia Zhou",
    "section": "",
    "text": "We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisual tags (e.g.,barcodes, QR codes) are ubiquitous in modern day life, though they rely on obtrusive geometric patterns to encode data, degrading the overall user experience. We propose a new paradigm of passive visual tags which utilizes light polarization to imperceptibly encode data using cheap, widely-available components. The tag and its data can be extracted from background scenery using off-the-shelf cameras with inexpensive LCD shutters attached atop camera lenses. We examine the feasibility of this design with real-world experiments. Initial results show zero bit errors at distances up to 3.0~m, an angular-detection range of 110’’, and robustness to manifold ambient light and occlusion scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.\n\n\n\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, presents the first demonstration of wireless, 3D localization across the air-water interface – eliminating the need for additional infrastructure on the water surface. Specifically, we propose a laser-based sensing system to enable aerial drones to directly locate underwater robots. The Sunflower system consists of a queen and a worker component on a drone and each tracked underwater robot, respectively. To achieve robust sensing, key system elements include (1) a pinhole- based sensing mechanism to address the sensing skew at air-water boundary and determine the incident angle on the worker, (2) a novel optical-fiber sensing ring to sense weak retroreflected light, (3) a laser-optimized backscatter communication design that exploits laser polarization to maximize retroreflected energy, and (4) the necessary models and algorithms for underwater sensing. Real- world experiments demonstrate that our Sunflower system achieves average localization error of 9.7 cm with ranges up to 3.8 m and is robust against ambient light interference and wave conditions.\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ability to communicate and sense across the air- water boundary is essential for efficient exploration and monitoring of the underwater world. Existing wireless solutions for communication and sensing typically focus on a single physical medium and fall short in achieving high-bandwidth communication and accurate sensing across the air-water interface without any relays on the water surface. We study the use of laser light in this context given its ability to effectively pass the air-water boundary. We present a holistic system framework to address practical challenges such as ambient light interference and environmental dynamics. The proposed AmphiLight framework achieves 5 Mbps bi-directional throughput and zero bit error rate with ranges up to 6.1 m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics. Our ongoing effort extends to realizing robust air-water sensing that enables aerial drones to track multiple underwater robots for topology planning and coordination.\n\n\n\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFace touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth)increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSenseintegrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch.Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model\n\n\n\n\n\nSep 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis paper presents a holistic system to scale up the teaching and learning of vocabulary words of American Sign Language (ASL). The system leverages the most recent mixed-reality technology to allow the user to perceive her own hands in an immersive learning environment with first- and third-person views for motion demonstration and practice. Precise motion sensing is used to record and evaluate motion, providing real-time feedback tailored to the specific learner. As part of this evaluation, learner motions are matched to features derived from the Hamburg Notation System (HNS) developed by sign-language linguists. We develop a prototype to evaluate the efficacy of mixed-reality-based interactive motion teaching. Results with 60 participants show a statistically significant improvement in learning ASL signs when using our system, in comparison to traditional desktop-based, non-interactive learning. We expect this approach to ultimately allow teaching and guided practice of thousands of signs.\n\n\n\n\n\nDec 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe propose a compact noninvasive glucose monitoring system using polarized light, where a user simply needs to place her palm on the device for measuring her current glucose concentration level. The primary innovation of our system is the ability to minimize light scattering from the skin and extract weak changes in light polarization to estimate glucose concentration, all using low-cost hardware. Our system exploits multiple wavelengths and light intensity levels to mitigate the effect of user diversity and confounding factors (e.g., collagen and elastin in the dermis). It then infers glucose concentration using a generic learning model, thus no additional calibration is needed. We design and fabricate a compact (17 cm × 10 cm × 5 cm) and low-cost (i.e., &lt;$250) prototype using off-the-shelf hardware. We evaluate our system with 41 diabetic patients and 9 healthy participants. In comparison to a continuous glucose monitor approved by U.S. Food and Drug Administration (FDA), 89% of our results are within zone A (clinically accurate) of the Clarke Error Grid. The absolute relative difference (ARD) is 10% . The r and p values of the Pearson correlation coefficients between our predicted glucose concentration and reference glucose concentration are 0.91 and 1.6 × 10−143, respectively. These errors are comparable with FDA-approved glucose sensors, which achieve ≈90% clinical accuracy with a 10% mean ARD.\n\n\n\n\n\nNov 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAir-water communication is fundamental for efficient underwater operations, such as environmental monitoring, surveying, or coordinating of heterogeneous aerial and underwater systems. Existing wireless techniques mostly focus on a single physical medium and fall short in achieving high-bandwidth bidirectional communication across the air-water interface. We propose a bidirectional, direct air-water wireless communication link based on laser light, capable of (1) adapting to water dynamics with ultrasonic sensing and (2) steering within a full 3D hemisphere using only a MEMS mirror and passive optical elements. In real-world experiments, our system achieves static throughputs up to 5.04 Mbps, zero-BER transmission ranges up to 6.1m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics.\n\n\n\n\n\nNov 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this paper we describe and characterize the largest Wi-Fi network trace ever published: spanning seven years, approximately 3000 distinct access points, 40,000 authenticated users, and 600,000 distinct Wi-Fi stations. The 7TB of raw data are pre-processed into connection sessions, which are made available for the research community. We describe the methods used to capture and process the traces, and characterize the most prominent trends and changes during the seven-year span of the trace. Furthermore, this Wi-Fi network covers the campus of Dartmouth College, the same campus detailed a decade earlier in seminal papers about that network and its users’ network behavior. We thus are able to comment on changes in patterns of usage, connection, and mobility in Wi-Fi deployments.\n\n\n\n\n\nNov 1, 2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#top-publications-1",
    "href": "index.html#top-publications-1",
    "title": "Xia Zhou",
    "section": "Top Publications",
    "text": "Top Publications\npath: publication\ntype: page\nsort: \"date desc\"\nlimit: 10\n:contentReference[oaicite:2]{index=2}\n\n**Notes:**\n\n- **Directory Structure**: :contentReference[oaicite:4]{index=4}:contentReference[oaicite:6]{index=6}\n\n- **Front Matter**: :contentReference[oaicite:8]{index=8}:contentReference[oaicite:10]{index=10}\n\n- **Listing Block**: :contentReference[oaicite:12]{index=12}:contentReference[oaicite:14]{index=14}\n\nFeel free to customize the layout or styling further to match your preferences. If you need assistance with additional features or customization, don't hesitate to ask!\n::contentReference[oaicite:15]{index=15}"
  }
]