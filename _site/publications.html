<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Publications – Xia Zhou</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-listing/list.min.js"></script>
<script src="site_libs/quarto-listing/quarto-listing.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d3f433bb8225445b39e21615b14794d2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-date','listing-title','listing-author','listing-image','listing-description',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 25,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-date","listing-title","listing-author","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Xia Zhou</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./publications.html" aria-current="page"> 
<span class="menu-text">Publications</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Publications</h1>
</div>



<div class="quarto-title-meta column-page">

    
  
    
  </div>
  


</header>


<section id="publications" class="level2">
<h2 class="anchored" data-anchor-id="publications">Publications</h2>



</section>

<div class="quarto-listing quarto-listing-container-default" id="listing-listing">
<div class="list quarto-listing-default">
<div class="quarto-post image-right" data-index="0" data-listing-file-modified-sort="1748580109639" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/chi20-threadsense/index.html" class="no-external">

<img loading="lazy" src="./publication/chi20-threadsense/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/chi20-threadsense/index.html" class="no-external">ThreadSense: Locating Touch on an Extremely Thin Interactive Thread</a>
</h3>
<div class="delink listing-description"><a href="./publication/chi20-threadsense/index.html" class="no-external">
<p>We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/chi20-threadsense/index.html" class="no-external">
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="1" data-listing-file-modified-sort="1748580109738" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/hotmobile20-polartag/index.html" class="no-external">

<img loading="lazy" src="./publication/hotmobile20-polartag/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotmobile20-polartag/index.html" class="no-external">PolarTag: Invisible Data with Light Polarization</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotmobile20-polartag/index.html" class="no-external">
<p>Visual tags (e.g.,barcodes, QR codes) are ubiquitous in modern day life, though they rely on obtrusive geometric patterns to encode data, degrading the overall user experience. We propose a new paradigm of passive visual tags which utilizes light polarization to imperceptibly encode data using cheap, widely-available components. The tag and its data can be extracted from background scenery using off-the-shelf cameras with inexpensive LCD shutters attached atop camera lenses. We examine the feasibility of this design with real-world experiments. Initial results show zero bit errors at distances up to 3.0~m, an angular-detection range of 110’’, and robustness to manifold ambient light and occlusion scenarios.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/hotmobile20-polartag/index.html" class="no-external">
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="2" data-listing-date-sort="1658894400000" data-listing-file-modified-sort="1748580109680" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/cogsci22-sl/index.html" class="no-external">

<img loading="lazy" src="./publication/cogsci22-sl/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/cogsci22-sl/index.html" class="no-external">Overlapping Semantic Representations of Sign and Speech in Novice Sign Language Learners</a>
</h3>
<div class="delink listing-description"><a href="./publication/cogsci22-sl/index.html" class="no-external">
<p>The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/cogsci22-sl/index.html" class="no-external">
<div class="listing-date">
Jul 27, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="3" data-listing-date-sort="1656388800000" data-listing-file-modified-sort="1748580109659" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobisys22-sunflower/index.html" class="no-external">

<img loading="lazy" src="./publication/mobisys22-sunflower/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobisys22-sunflower/index.html" class="no-external">Sunflower: Locating Underwater Robots From the Air</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobisys22-sunflower/index.html" class="no-external">
<p>Locating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, presents the first demonstration of wireless, 3D localization across the air-water interface – eliminating the need for additional infrastructure on the water surface. Specifically, we propose a laser-based sensing system to enable aerial drones to directly locate underwater robots. The Sunflower system consists of a queen and a worker component on a drone and each tracked underwater robot, respectively. To achieve robust sensing, key system elements include (1) a pinhole- based sensing mechanism to address the sensing skew at air-water boundary and determine the incident angle on the worker, (2) a novel optical-fiber sensing ring to sense weak retroreflected light, (3) a laser-optimized backscatter communication design that exploits laser polarization to maximize retroreflected energy, and (4) the necessary models and algorithms for underwater sensing. Real- world experiments demonstrate that our Sunflower system achieves average localization error of 9.7 cm with ranges up to 3.8 m and is robust against ambient light interference and wave conditions.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobisys22-sunflower/index.html" class="no-external">
<div class="listing-date">
Jun 28, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="4" data-listing-date-sort="1641358800000" data-listing-file-modified-sort="1748580109655" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/comsnets22-light/index.html" class="no-external">

<img loading="lazy" src="./publication/comsnets22-light/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/comsnets22-light/index.html" class="no-external">Air-Water Communication and Sensing with Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/comsnets22-light/index.html" class="no-external">
<p>The ability to communicate and sense across the air- water boundary is essential for efficient exploration and monitoring of the underwater world. Existing wireless solutions for communication and sensing typically focus on a single physical medium and fall short in achieving high-bandwidth communication and accurate sensing across the air-water interface without any relays on the water surface. We study the use of laser light in this context given its ability to effectively pass the air-water boundary. We present a holistic system framework to address practical challenges such as ambient light interference and environmental dynamics. The proposed AmphiLight framework achieves 5 Mbps bi-directional throughput and zero bit error rate with ranges up to 6.1 m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics. Our ongoing effort extends to realizing robust air-water sensing that enables aerial drones to track multiple underwater robots for topology planning and coordination.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/comsnets22-light/index.html" class="no-external">
<div class="listing-date">
Jan 5, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="5" data-listing-date-sort="1630468800000" data-listing-file-modified-sort="1748580109648" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp21-facesense/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp21-facesense/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp21-facesense/index.html" class="no-external">FaceSense: Sensing Face Touch with an Ear-worn System</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp21-facesense/index.html" class="no-external">
<p>Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth)increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSenseintegrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch.Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp21-facesense/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2021
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="6" data-listing-date-sort="1606798800000" data-listing-file-modified-sort="1748580109711" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp20-motion/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp20-motion/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp20-motion/index.html" class="no-external">Teaching American Sign Language in Mixed Reality</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp20-motion/index.html" class="no-external">
<p>This paper presents a holistic system to scale up the teaching and learning of vocabulary words of American Sign Language (ASL). The system leverages the most recent mixed-reality technology to allow the user to perceive her own hands in an immersive learning environment with first- and third-person views for motion demonstration and practice. Precise motion sensing is used to record and evaluate motion, providing real-time feedback tailored to the specific learner. As part of this evaluation, learner motions are matched to features derived from the Hamburg Notation System (HNS) developed by sign-language linguists. We develop a prototype to evaluate the efficacy of mixed-reality-based interactive motion teaching. Results with 60 participants show a statistically significant improvement in learning ASL signs when using our system, in comparison to traditional desktop-based, non-interactive learning. We expect this approach to ultimately allow teaching and guided practice of thousands of signs.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp20-motion/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="7" data-listing-date-sort="1605502800000" data-listing-file-modified-sort="1748580109769" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sensys20-glucose/index.html" class="no-external">

<img loading="lazy" src="./publication/sensys20-glucose/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sensys20-glucose/index.html" class="no-external">Noninvasive Glucose Monitoring Using Polarized Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/sensys20-glucose/index.html" class="no-external">
<p>We propose a compact noninvasive glucose monitoring system using polarized light, where a user simply needs to place her palm on the device for measuring her current glucose concentration level. The primary innovation of our system is the ability to minimize light scattering from the skin and extract weak changes in light polarization to estimate glucose concentration, all using low-cost hardware. Our system exploits multiple wavelengths and light intensity levels to mitigate the effect of user diversity and confounding factors (e.g., collagen and elastin in the dermis). It then infers glucose concentration using a generic learning model, thus no additional calibration is needed. We design and fabricate a compact (17 cm × 10 cm × 5 cm) and low-cost (i.e., &lt;$250) prototype using off-the-shelf hardware. We evaluate our system with 41 diabetic patients and 9 healthy participants. In comparison to a continuous glucose monitor approved by U.S. Food and Drug Administration (FDA), 89% of our results are within zone A (clinically accurate) of the Clarke Error Grid. The absolute relative difference (ARD) is 10% . The r and p values of the Pearson correlation coefficients between our predicted glucose concentration and reference glucose concentration are 0.91 and 1.6 × 10−143, respectively. These errors are comparable with FDA-approved glucose sensors, which achieve ≈90% clinical accuracy with a 10% mean ARD.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/sensys20-glucose/index.html" class="no-external">
<div class="listing-date">
Nov 16, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="8" data-listing-date-sort="1604203200000" data-listing-file-modified-sort="1748580109751" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/nsdi20-amphilight/index.html" class="no-external">

<img loading="lazy" src="./publication/nsdi20-amphilight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/nsdi20-amphilight/index.html" class="no-external">AmphiLight: Direct Air-Water Communication with Laser Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/nsdi20-amphilight/index.html" class="no-external">
<p>Air-water communication is fundamental for efficient underwater operations, such as environmental monitoring, surveying, or coordinating of heterogeneous aerial and underwater systems. Existing wireless techniques mostly focus on a single physical medium and fall short in achieving high-bandwidth bidirectional communication across the air-water interface. We propose a bidirectional, direct air-water wireless communication link based on laser light, capable of (1) adapting to water dynamics with ultrasonic sensing and (2) steering within a full 3D hemisphere using only a MEMS mirror and passive optical elements. In real-world experiments, our system achieves static throughputs up to 5.04 Mbps, zero-BER transmission ranges up to 6.1m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/nsdi20-amphilight/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="9" data-listing-date-sort="1572580800000" data-listing-file-modified-sort="1748580109702" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/LongitudinalAnalysis/index.html" class="no-external">

<img loading="lazy" src="./publication/LongitudinalAnalysis/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/LongitudinalAnalysis/index.html" class="no-external">Longitudinal Analysis of a Campus Wi-Fi Network</a>
</h3>
<div class="delink listing-description"><a href="./publication/LongitudinalAnalysis/index.html" class="no-external">
<p>In this paper we describe and characterize the largest Wi-Fi network trace ever published: spanning seven years, approximately 3000 distinct access points, 40,000 authenticated users, and 600,000 distinct Wi-Fi stations. The 7TB of raw data are pre-processed into connection sessions, which are made available for the research community. We describe the methods used to capture and process the traces, and characterize the most prominent trends and changes during the seven-year span of the trace. Furthermore, this Wi-Fi network covers the campus of Dartmouth College, the same campus detailed a decade earlier in seminal papers about that network and its users’ network behavior. We thus are able to comment on changes in patterns of usage, connection, and mobility in Wi-Fi deployments.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/LongitudinalAnalysis/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2019
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="10" data-listing-date-sort="1572580800000" data-listing-file-modified-sort="1748580109725" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp19-fabric/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp19-fabric/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp19-fabric/index.html" class="no-external">Reconstructing Human Joint Motion with Computational Fabrics</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp19-fabric/index.html" class="no-external">
<p>Accurate and continuous monitoring of joint rotational motion is crucial for a wide range of applications such as physical rehabilitation [6, 85] and motion training [22, 54, 68]. Existing motion capture systems, however, either need instrumentation of the environment, or fail to track arbitrary joint motion, or impose wearing discomfort by requiring rigid electrical sensors right around the joint area. This work studies the use of everyday fabrics as a flexible and soft sensing medium to monitor joint angular motion accurately and reliably. Specifically we focus on the primary use of conductive stretchable fabrics to sense the skin deformation during joint motion and infer the joint rotational angle. We tackle challenges of fabric sensing originated by the inherent properties of elastic materials by leveraging two types of sensing fabric and characterizing their properties based on models in material science. We apply models from bio-mechanics to infer joint angles and propose the use of dual strain sensing to enhance sensing robustness against user diversity and fabric position offsets. We fabricate prototypes using off-the-shelf fabrics and micro-controller. Experiments with ten participants show 9.69° median angular error in tracking joint angle and its sensing robustness across various users and activities.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp19-fabric/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2019
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="11" data-listing-date-sort="1538366400000" data-listing-file-modified-sort="1748580109747" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom18-eye/index.html" class="no-external">

<img loading="lazy" src="./publication/mobicom18-eye/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom18-eye/index.html" class="no-external">Battery-Free Eye Tracker on Glasses</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom18-eye/index.html" class="no-external">
<p>This paper presents a battery-free wearable eye tracker that tracks both the 2D position and diameter of a pupil based on its light absorption property. With a few near-infrared (NIR) lights and photodiodes around the eye, NIR lights sequentially illuminate the eye from various directions while photodiodes sense spatial patterns of reflected light, which are used to infer pupil’s position and diameter on the fly via a lightweight inference algorithm. The system also exploits characteristics of different eye movement stages and adjusts its sensing and computation accordingly for further energy savings. A prototype is built with off-the-shelf hardware components and integrated into a regular pair of glasses. Experiments with 22 participants show that the system achieves 0.8-mm mean error in tracking pupil position (2.3 mm at the 95th percentile) and 0.3-mm mean error in tracking pupil diameter (0.9 mm at the 95th percentile) at 120-Hz output frame rate, consuming 395 µW mean power supplied by two small, thin solar cells on glasses side arms.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom18-eye/index.html" class="no-external">
<div class="listing-date">
Oct 1, 2018
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="12" data-listing-date-sort="1538366400000" data-listing-file-modified-sort="1748580109756" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/uist18-selfpower/index.html" class="no-external">

<img loading="lazy" src="./publication/uist18-selfpower/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/uist18-selfpower/index.html" class="no-external">Self-Powered Gesture Recognition with Ambient Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/uist18-selfpower/index.html" class="no-external">
<p>We present a self-powered module for gesture recognition that utilizes small, low-cost photodiodes for both energy harvesting and gesture sensing. Operating in the photovoltaic mode, photodiodes harvest energy from ambient light. In the meantime, the instantaneously harvested power from individual photodiodes is monitored and exploited as clues for sensing finger gestures in proximity. Harvested power from all photodiodes are aggregated to drive the whole gesture-recognition module including the micro-controller running the recognition algorithm. We design robust, lightweight algorithm to recognize finger gestures in the presence of ambient light fluctuations. We fabricate two prototypes to facilitate user’s interaction with smart glasses and smart watch. Results show 99.7%/98.3% overall precision/recall in recognizing five gestures on glasses and 99.2%/97.5% precision/recall in recognizing seven gestures on the watch. The system consumes 34.6 µW/74.3 µW for the glasses/watch and thus can be powered by the energy harvested from ambient light. We also test system’s robustness under varying light intensities, light directions, and ambient light fluctuations, where the system maintains high recognition accuracy (&gt; 96%) in all tested settings.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/uist18-selfpower/index.html" class="no-external">
<div class="listing-date">
Oct 1, 2018
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="13" data-listing-date-sort="1527825600000" data-listing-file-modified-sort="1748580109695" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobisys18-polar/index.html" class="no-external">

<img loading="lazy" src="./publication/mobisys18-polar/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobisys18-polar/index.html" class="no-external">Augmenting Indoor Inertial Tracking with Polarized Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobisys18-polar/index.html" class="no-external">
<p>Inertial measurement unit (IMU) has long suffered from the problem of integration drift, where sensor noises accumulate quickly and cause fast-growing tracking errors. Existing methods for calibrating IMU tracking either require human in the loop, or need energy-consuming cameras, or suffer from coarse tracking granularity. We propose to augment indoor inertial tracking by reusing existing indoor luminaries to project a static light polarization pattern in the space. This pattern is imperceptible to human eyes and yet through a polarizer, it becomes detectable by a color sensor, and thus can serve as fine-grained optical landmarks that constrain and correct IMU’s integration drift and boost tracking accuracy. Exploiting the birefringence optical property of transparent tapes – a low-cost and easily-accessible material – we realize the polarization pattern by simply adding to existing light cover a thin polarizer film with transparent tape stripes glued atop. When fusing with IMU sensor signals, the light pattern enables robust, accurate and low-power motion tracking. Meanwhile, our approach entails low deployment overhead by reusing existing lighting infrastructure without needing an active modulation unit. We build a prototype of our light cover and the sensing unit using off-the-shelf components. Experiments show 4.3 cm median error for 2D tracking and 10 cm for 3D tracking, as well as its robustness in diverse settings.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobisys18-polar/index.html" class="no-external">
<div class="listing-date">
Jun 1, 2018
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="14" data-listing-date-sort="1519880400000" data-listing-file-modified-sort="1748580109692" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp18-protractor/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp18-protractor/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp18-protractor/index.html" class="no-external">Measuring Interaction Proxemics with Wearable Light Tags</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp18-protractor/index.html" class="no-external">
<p>The proxemics of social interactions (e.g., body distance, relative orientation) influences many aspects of our everyday life: from patients’ reactions to interaction with physicians, successes in job interviews, to effective teamwork. Traditionally, interaction proxemics has been studied via questionnaires and participant observations, imposing high burden on users, low scalability and precision, and often biases. In this paper we present Protractor, a novel wearable technology for measuring interaction proxemics as part of non-verbal behavior cues with fine granularity. Protractor employs near-infrared light to monitor both the distance and relative body orientation of interacting users. We leverage the characteristics of near-infrared light (i.e., line-of-sight propagation) to accurately and reliably identify interactions; a pair of collocated photodiodes aid the inference of relative interaction angle and distance. We achieve robustness against temporary blockage of the light channel (e.g., by the user’s hand or clothes) by designing sensor fusion algorithms that exploit inertial sensors to obviate the absence of light tracking results. We fabricated Protractor tags and conducted real-world experiments. Results show its accuracy in tracking body distances and relative angles. The framework achieves less than 6° error 95% of the time for measuring relative body orientation and 2.3-cm - 4.9-cm mean error in estimating interaction distance. We deployed Protractor tags to track user’s non-verbal behaviors when conducting collaborative group tasks. Results with 64 participants show that distance and angle data from Protractor tags can help assess individual’s task role with 84.9% accuracy, and identify task timeline with 93.2% accuracy.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp18-protractor/index.html" class="no-external">
<div class="listing-date">
Mar 1, 2018
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="15" data-listing-date-sort="1512104400000" data-listing-file-modified-sort="1748578831012" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/buildsys17-3dprint/index.html" class="no-external">

<img loading="lazy" src="./publication/buildsys17-3dprint/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/buildsys17-3dprint/index.html" class="no-external">Customizing Indoor Wireless Coverage via 3D-Fabricated Reflectors</a>
</h3>
<div class="delink listing-description"><a href="./publication/buildsys17-3dprint/index.html" class="no-external">
<p>Judicious control of indoor wireless coverage is crucial in built environments. It enhances signal reception, reduces harmful interference, and raises the barrier for malicious attackers. Existing methods are either costly, vulnerable to attacks, or hard to configure. We present a low-cost, secure, and easy-to-configure approach that uses an easily-accessible, 3D-fabricated reflector to customize wireless coverage. With input on coarse-grained environment setting and preferred coverage (e.g., areas with signals to be strengthened or weakened), the system computes an optimized reflector shape tailored to the given environment. The user simply 3D prints the reflector and places it around a Wi-Fi access point to realize the target coverage. We conduct experiments to examine the efficacy and limits of optimized reflectors in different indoor settings. Results show that optimized reflectors coexist with a variety of Wi-Fi APs and correctly weaken or enhance signals in target areas by up to 10 or 6 dB, resulting in throughput changes by up to -63.3% or 55.1%.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/buildsys17-3dprint/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="16" data-listing-date-sort="1509508800000" data-listing-file-modified-sort="1748662109230" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sensys17-ligaze/index.html" class="no-external">

<img loading="lazy" src="./publication/sensys17-ligaze/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sensys17-ligaze/index.html" class="no-external">Ultra-Low Power Gaze Tracking for Virtual Reality</a>
</h3>
<div class="delink listing-description"><a href="./publication/sensys17-ligaze/index.html" class="no-external">
<p>Tracking user’s eye fixation direction is crucial to virtual reality (VR): it eases user’s interaction with the virtual scene and enables intelligent rendering to improve user’s visual experiences and save system energy. Existing techniques commonly rely on cameras and active infrared emitters, making them too expensive and power-hungry for VR headsets (especially mobile VR headsets). We present LiGaze, a low-cost, low-power approach to gaze tracking tailored to VR. It relies on a few low-cost photodiodes, eliminating the need for cameras and active infrared emitters. Reusing light emitted from the VR screen, LiGaze leverages photodiodes around a VR lens to measure reflected screen light in different directions. It then infers gaze direction by exploiting pupil’s light absorption property. The core of LiGaze is to deal with screen light dynamics and extract changes in reflected light related to pupil movement. LiGaze infers a 3D gaze vector on the fly using a lightweight regression algorithm. We design and fabricate a LiGaze prototype using off-the-shelf photodiodes. Our comparison to a commercial VR eye tracker (FOVE) shows that LiGaze achieves 6.3° and 10.1° mean within-user and cross-user accuracy. Its sensing and computation consume 791μW in total and thus can be completely powered by a credit-card sized solar cell harvesting energy from indoor lighting. LiGaze’s simplicity and ultra-low power make it applicable in a wide range of VR headsets to better unleash VR’s potential.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/sensys17-ligaze/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="17" data-listing-date-sort="1506830400000" data-listing-file-modified-sort="1748580109774" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/uist17-pyro/index.html" class="no-external">

<img loading="lazy" src="./publication/uist17-pyro/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/uist17-pyro/index.html" class="no-external">Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing.</a>
</h3>
<div class="delink listing-description"><a href="./publication/uist17-pyro/index.html" class="no-external">
<p>We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro’s robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/uist17-pyro/index.html" class="no-external">
<div class="listing-date">
Oct 1, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="18" data-listing-date-sort="1504238400000" data-listing-file-modified-sort="1748580109674" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/vlcs17-inertial/index.html" class="no-external">

<img loading="lazy" src="./publication/vlcs17-inertial/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/vlcs17-inertial/index.html" class="no-external">Position: Augmenting Inertial Tracking with Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/vlcs17-inertial/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/vlcs17-inertial/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="19" data-listing-date-sort="1501560000000" data-listing-file-modified-sort="1748580109652" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp17-aili/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp17-aili/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp17-aili/index.html" class="no-external">Reconstructing Hand Poses Using Visible Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp17-aili/index.html" class="no-external">
<p>Free-hand gestural input is essential for emerging user interactions. We present Aili, a table lamp reconstructing a 3D hand skeleton in real time, requiring neither cameras nor on-body sensing devices. Aili consists of an LED panel in a lampshade and a few low-cost photodiodes embedded in the lamp base. To reconstruct a hand skeleton, Aili combines 2D binary blockage maps from vantage points of different photodiodes, which describe whether a hand blocks light rays from individual LEDs to all photodiodes. Empowering a table lamp with sensing capability, Aili can be seamlessly integrated into the existing environment. Relying on such low-level cues, Aili entails lightweight computation and is inherently privacy-preserving. We build and evaluate an Aili prototype. Results show that Aili’s algorithm reconstructs a hand pose within 7.2 ms on average, with 10.2° mean angular deviation and 2.5-mm mean translation deviation in comparison to Leap Motion. We also conduct user studies to examine the privacy issues of Leap Motion and solicit feedback on Aili’s privacy protection. We conclude by demonstrating various interaction applications Aili enables.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp17-aili/index.html" class="no-external">
<div class="listing-date">
Aug 1, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="20" data-listing-date-sort="1498968000000" data-listing-file-modified-sort="1748580109765" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobilehealth17/index.html" class="no-external">

<img loading="lazy" src="./publication/mobilehealth17/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobilehealth17/index.html" class="no-external">StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobilehealth17/index.html" class="no-external">
<p>Much of the stress and strain of student life remains hidden. The StudentLife continuous sensing app assesses the day-to-day and week-by-week impact of workload on stress, sleep, activity, mood, sociability, mental well-being and academic performance of a single class of 48 students across a 10 week term at Dartmouth College using Android phones. Results from the StudentLife study show a number of significant correlations between the automatic objective sensor data from smartphones and mental health and educational outcomes of the student body. We also identify a Dartmouth term lifecycle in the data that shows students start the term with high positive affect and conversation levels, low stress, and healthy sleep and daily activity patterns. As the term progresses and the workload increases, stress appreciably rises while positive affect, sleep, conversation and activity drops off. The StudentLife dataset is publicly available on the web.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobilehealth17/index.html" class="no-external">
<div class="listing-date">
Jul 2, 2017
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="21" data-listing-date-sort="1477972800000" data-listing-file-modified-sort="1748580109705" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sensys16-specmonitor/index.html" class="no-external">

<img loading="lazy" src="./publication/sensys16-specmonitor/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sensys16-specmonitor/index.html" class="no-external">Empirical Validation of Commodity Spectrum Monitoring</a>
</h3>
<div class="delink listing-description"><a href="./publication/sensys16-specmonitor/index.html" class="no-external">
<p>We describe our efforts to empirically validate a distributed spectrum monitoring system built on commodity smartphones and embedded low-cost spectrum sensors. This system enables real-time spectrum sensing, identifies and locates active transmitters, and generates alarm events when detecting anomalous transmitters. To evaluate the feasibility of such a platform, we perform detailed experiments using a prototype hardware platform using smartphones and RTL dongles. We identify multiple sources of error in the sensing results and the end-user overhead (i.e.&nbsp;smartphone energy draw). We propose and implement a variety of techniques to identify and overcome errors and uncertainty in the data, and to reduce energy consumption. Our work demonstrates the basic viability of user-driven spectrum monitoring on commodity devices.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/sensys16-specmonitor/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2016
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="22" data-listing-date-sort="1475294400000" data-listing-file-modified-sort="1748580109735" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom16-darklight/index.html" class="no-external">

<img loading="lazy" src="./publication/mobicom16-darklight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom16-darklight/index.html" class="no-external">The DarkLight Rises: Visible Light Communication in the Dark</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom16-darklight/index.html" class="no-external">
<p>Visible Light Communication (VLC) emerges as a new wireless communication technology with appealing benefits not present in radio communication. However, current VLC designs commonly require LED lights to emit perceptible light beams, which greatly limits the applicable scenarios of VLC (e.g., in a sunny day when indoor lighting is not needed), and brings high energy overhead and unpleasant visual experiences for mobile devices to transmit data using VLC. We design and develop DarkLight, a new VLC primitive that allows light-based communication to be sustained even when LEDs emit extremely-low luminance. The key idea is to encode data into ultra-short, imperceptible light pulses. We tackle challenges in circuit designs, data encoding/decoding schemes, and DarkLight networking, to efficiently generate and reliably detect ultra-short light pulses using off-the-shelf, low-cost LEDs and photodiodes. Our DarkLight prototype supports 1.3-m distance with 1.6-Kbps data rate. By loosening up VLC’s reliance on visible light beams, DarkLight presents an unconventional direction of VLC and fundamentally broadens VLC’s application scenarios.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom16-darklight/index.html" class="no-external">
<div class="listing-date">
Oct 1, 2016
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="23" data-listing-date-sort="1472702400000" data-listing-file-modified-sort="1748580109741" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/wintech16-drone/index.html" class="no-external">

<img loading="lazy" src="./publication/wintech16-drone/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/wintech16-drone/index.html" class="no-external">Automating 3D Wireless Measurements with Drones</a>
</h3>
<div class="delink listing-description"><a href="./publication/wintech16-drone/index.html" class="no-external">
<p>Wireless signals and networks are ubiquitous. Though more reliable than ever, wireless networks still struggle with weak coverage, blind spots, and interference. Having a strong understanding of wireless signal propagation is essential for increasing coverage, optimizing performance, and minimizing interference for wireless networks. Extensive studies have analyzed the propagation of wireless signals and proposed theoretical models to simulate wireless signal propagation. Unfortunately, models of signal propagation are often not accurate in reality. Real-world signal measurements are required for validation. Existing methods for collecting wireless measurements either involve researchers walking to each location of interest and manually collecting measurements, or place sensors at each measurement location. As such, they require large amounts of time and effort and can be costly. We propose DroneSense, a system for measuring wireless signals in the 3D space using autonomous drones. Drone-Sense reduces the time and effort required for measurement collection, and is affordable and accessible to all users. It provides researchers with an efficient method to quickly analyze wireless coverage and test their wireless propagation models.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/wintech16-drone/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2016
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="24" data-listing-date-sort="1464753600000" data-listing-file-modified-sort="1748580109625" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobisys16-starlight/index.html" class="no-external">

<img loading="lazy" src="./publication/mobisys16-starlight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobisys16-starlight/index.html" class="no-external">Practical Human Sensing in the Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobisys16-starlight/index.html" class="no-external">
<p>We present StarLight, an infrastructure-based sensing system that reuses light emitted from ceiling LED panels to reconstruct fine-grained user skeleton postures continuously in real time. It relies on only a few (e.g., 20) photodiodes placed at optimized locations to passively capture low-level visual clues (light blockage information), with neither cameras capturing sensitive images, nor on-body devices, nor electromagnetic interference. It then aggregates the blockage information of a large number of light rays from LED panels and identifies best-fit 3D skeleton postures. StarLight greatly advances the prior light-based sensing design by dramatically reducing the number of intrusive sensors, overcoming furniture blockage, and supporting user mobility. We build and deploy StarLight in a 3.6 m x 4.8 m office room, with customized 20 LED panels and 20 photodiodes. Experiments show that StarLight achieves 13.6° mean angular error for five body joints and reconstructs a mobile skeleton at a high frame rate (40 FPS). StarLight enables a new unobtrusive sensing paradigm to augment today’s mobile sensing for continuous and accurate behavioral monitoring.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobisys16-starlight/index.html" class="no-external">
<div class="listing-date">
Jun 1, 2016
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="25" data-listing-date-sort="1454302800000" data-listing-file-modified-sort="1748580109744" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/hotmobile16-darkvlc/index.html" class="no-external">

<img loading="lazy" data-src="./publication/hotmobile16-darkvlc/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotmobile16-darkvlc/index.html" class="no-external">Lighting Up the Internet of Things with DarkVLC</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotmobile16-darkvlc/index.html" class="no-external">
<p>Visible Light Communication (VLC) holds a great potential to solve the spectrum crunch problem and to provide scalable connectivity to zillions of mobile and IoT devices. However, VLC commonly requires LED lights to be on, which fundamentally limits the applicable scenarios of VLC and makes VLC less attractive to mobile and IoT devices with tight energy budget. We present DarkVLC, a new VLC primitive that allows the VLC link to be sustained even when the LED lights appear dark or off. The key idea is to encode data into ultra-short light pulses imperceptible to human eyes yet detectable by devices equipped with photodiodes. Realizing DarkVLC faces several challenges to generate and deal with the ultra-short light pulses reliably. We describe our preliminary efforts to tackle these challenges and build a DarkVLC prototype using off-the-shelf LEDs and low-cost photodiodes. DarkVLC fundamentally broadens the application scenarios of VLC and provides a new ultra-low power, always-on connectivity affordable for mobile and IoT devices.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/hotmobile16-darkvlc/index.html" class="no-external">
<div class="listing-date">
Feb 1, 2016
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="26" data-listing-date-sort="1441080000000" data-listing-file-modified-sort="1748580109682" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mcss15-facelogging/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mcss15-facelogging/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mcss15-facelogging/index.html" class="no-external">Using opportunistic face logging from smartphone to infer mental health: challenges and future directions</a>
</h3>
<div class="delink listing-description"><a href="./publication/mcss15-facelogging/index.html" class="no-external">
<p>We discuss the opportunistic face logging project that collected 5811 opportunistic photos using the front-facing camera from the smartphones of 37 participants over a 10-week period. We present our experiences using computer vision and human labeling approaches on the photos to assess mental health of the participants. Finally, we discuss our insights, challenges, and future directions.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mcss15-facelogging/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="27" data-listing-date-sort="1441080000000" data-listing-file-modified-sort="1748580109772" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mcss15-mpsm/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mcss15-mpsm/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mcss15-mpsm/index.html" class="no-external">The Mobile Photographic Stress Meter (MPSM): A New Way to Measure Stress Using Images</a>
</h3>
<div class="delink listing-description"><a href="./publication/mcss15-mpsm/index.html" class="no-external">
<p>Stress is an important aspect of human psychology. Many measures of stress have been developed over the years, but different issues exist for most of these measures. This study introduces the Mobile Photographic Stress Meter (MPSM), a new way to measure stress in which a user simply selects an image that best captures his or her stress level. Such a tool allows researchers to quickly measure stress in real time and in natural environments less onerous for the user. Our results show that MPSM is a valid measure of stress. Users find it easy and actually enjoyable to use. Our results also show that MPSM is strongly correlated with the Perceived Stress Scale, a validated multi-item stress scale. The correlation has an r-value of 0.5559 and a p-value of less than 0.001. We conclude that MPSM can be applied effectively in research experiments and advance the research on stress.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mcss15-mpsm/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="28" data-listing-date-sort="1427860800000" data-listing-file-modified-sort="1748580109688" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/hotw15-wiprint/index.html" class="no-external">

<img loading="lazy" data-src="./publication/hotw15-wiprint/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotw15-wiprint/index.html" class="no-external">3D Printing Your Wireless Coverage</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotw15-wiprint/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/hotw15-wiprint/index.html" class="no-external">
<div class="listing-date">
Apr 1, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="29" data-listing-date-sort="1425186000000" data-listing-file-modified-sort="1748580109672" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/vlcs15-id/index.html" class="no-external">

<img loading="lazy" data-src="./publication/vlcs15-id/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/vlcs15-id/index.html" class="no-external">Visible Light Knows Who You Are</a>
</h3>
<div class="delink listing-description"><a href="./publication/vlcs15-id/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/vlcs15-id/index.html" class="no-external">
<div class="listing-date">
Mar 1, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="30" data-listing-date-sort="1422766800000" data-listing-file-modified-sort="1748580109686" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom15-lisense/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobicom15-lisense/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom15-lisense/index.html" class="no-external">Human Sensing Using Visible Light Communication</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom15-lisense/index.html" class="no-external">
<p>We present LiSense, the first-of-its-kind system that enables both data communication and fine-grained, real-time human skeleton reconstruction using Visible Light Communication (VLC). LiSense uses shadows created by the human body from blocked light and reconstructs 3D human skeleton postures in real time. We overcome two key challenges to realize shadow-based human sensing. First, multiple lights on the ceiling lead to diminished and complex shadow patterns on the floor. We design light beacons enabled by VLC to separate light rays from different light sources and recover the shadow pattern cast by each individual light. Second, we design an efficient inference algorithm to reconstruct user postures using 2D shadow information with a limited resolution collected by photodiodes embedded in the floor. We build a 3 m x 3 m LiSense testbed using off-the-shelf LEDs and photodiodes. Experiments show that LiSense reconstructs the 3D user skeleton at 60 Hz in real time with 10 degrees mean angular error for five body joints.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom15-lisense/index.html" class="no-external">
<div class="listing-date">
Feb 1, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="31" data-listing-date-sort="1422680400000" data-listing-file-modified-sort="1748580109633" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp15-smartgpa/index.html" class="no-external">

<img loading="lazy" data-src="./publication/ubicomp15-smartgpa/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp15-smartgpa/index.html" class="no-external">SmartGPA: How Smartphones Can Assess and Predict Academic Performance of College Students</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp15-smartgpa/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp15-smartgpa/index.html" class="no-external">
<div class="listing-date">
Jan 31, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="32" data-listing-date-sort="1422594000000" data-listing-file-modified-sort="1748580109662" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp15-wiscan/index.html" class="no-external">

<img loading="lazy" data-src="./publication/ubicomp15-wiscan/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp15-wiscan/index.html" class="no-external">Low-Power Pervasive Wi-Fi Connectivity Using WiScan</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp15-wiscan/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp15-wiscan/index.html" class="no-external">
<div class="listing-date">
Jan 30, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="33" data-listing-date-sort="1422421200000" data-listing-file-modified-sort="1748580109667" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobisys15-hilight/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobisys15-hilight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobisys15-hilight/index.html" class="no-external">Real-Time Screen-Camera Communication Behind Any Scene</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobisys15-hilight/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/mobisys15-hilight/index.html" class="no-external">
<div class="listing-date">
Jan 28, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="34" data-listing-date-sort="1422334800000" data-listing-file-modified-sort="1748580109716" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/www15-wifilte/index.html" class="no-external">

<img loading="lazy" data-src="./publication/www15-wifilte/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/www15-wifilte/index.html" class="no-external">Energy and Performance of Smartphone Radio Bundling in Outdoor Environments</a>
</h3>
<div class="delink listing-description"><a href="./publication/www15-wifilte/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/www15-wifilte/index.html" class="no-external">
<div class="listing-date">
Jan 27, 2015
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="35" data-listing-date-sort="1409544000000" data-listing-file-modified-sort="1748580109677" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/hotw14-ivlc/index.html" class="no-external">

<img loading="lazy" data-src="./publication/hotw14-ivlc/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotw14-ivlc/index.html" class="no-external">Visible Light Networking and Sensing</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotw14-ivlc/index.html" class="no-external">
<p>We propose for the first time an integrated Visible Light Communication (iVLC) system, which combines scalable VLC networking and accurate VLC sensing of mobile users. To meet this goal, we envision using modulated LED lights for communications between networked devices, while at the same time using the very same lights to accurately identify and track users, and importantly, sense and infer their gestures (e.g., pointing to an object in the room) as a means of collecting user analytics and enabling interactions with objects in smart spaces. Enabling the iVLC vision requires reliable VLC networking and robust VLC sensing. We discuss the key research components and open challenges in realizing this vision. By combining VLC networking and sensing, iVLC opens the way for a new class of context-aware applications and a new HCI paradigm not possible before.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/hotw14-ivlc/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="36" data-listing-date-sort="1409544000000" data-listing-file-modified-sort="1748580109699" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp14-studentlife/index.html" class="no-external">

<img loading="lazy" data-src="./publication/ubicomp14-studentlife/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp14-studentlife/index.html" class="no-external">StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp14-studentlife/index.html" class="no-external">
<p>Much of the stress and strain of student life remains hidden. The StudentLife continuous sensing app assesses the day-to-day and week-by-week impact of workload on stress, sleep, activity, mood, sociability, mental well-being and academic performance of a single class of 48 students across a 10 week term at Dartmouth College using Android phones. Results from the StudentLife study show a number of significant correlations between the automatic objective sensor data from smartphones and mental health and educational outcomes of the student body. We also identify a Dartmouth term lifecycle in the data that shows students start the term with high positive affect and conversation levels, low stress, and healthy sleep and daily activity patterns. As the term progresses and the workload increases, stress appreciably rises while positive affect, sleep, conversation and activity drops off. The StudentLife dataset is publicly available on the web.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp14-studentlife/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="37" data-listing-date-sort="1406865600000" data-listing-file-modified-sort="1748580109642" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/hotw14-specmonitor/index.html" class="no-external">

<img loading="lazy" data-src="./publication/hotw14-specmonitor/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotw14-specmonitor/index.html" class="no-external">Towards Commoditized Real-time Spectrum Monitoring</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotw14-specmonitor/index.html" class="no-external">
<p>We are facing an increasingly difficult challenge in spectrum management: how to perform real-time spectrum monitoring with strong coverage of deployed regions. Today’s spectrum measurements are carried out by government employees driving around with specialized hardware that is usually bulky and expensive, making the task of gathering real-time, large-scale spectrum monitoring data extremely difficult and cost prohibitive. In this paper, we propose a solution to the spectrum monitoring problem by leveraging the power of the masses, i.e.&nbsp;millions of wireless users, using low-cost, commoditized spectrum monitoring hardware. We envision an ecosystem where crowdsourced smartphone users perform automated and continuous spectrum measurements using their mobile devices, and report the results to a monitoring agency in real-time. We perform an initial feasibility study to verify the efficacy of our mobile monitoring platform compared to that of conventional monitoring devices like USRP GNU radios. Results indicate that commoditized real-time spectrum monitoring is indeed feasible in the near future. We conclude by presenting a set of open challenges and potential directions for follow-up research.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/hotw14-specmonitor/index.html" class="no-external">
<div class="listing-date">
Aug 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="38" data-listing-date-sort="1404187200000" data-listing-file-modified-sort="1748580109665" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/vlcs14-hilight/index.html" class="no-external">

<img loading="lazy" data-src="./publication/vlcs14-hilight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/vlcs14-hilight/index.html" class="no-external">HiLight: Hiding Bits in Pixel Translucency Changes</a>
</h3>
<div class="delink listing-description"><a href="./publication/vlcs14-hilight/index.html" class="no-external">
<p>We present HiLight, a new form of unobtrusive screen-camera communication for off-theshelf smart devices. HiLight hides information underlying any images shown on an LED or OLED screen, and camera-equipped smart devices can fetch the information by turning their cameras to the screen. HiLight achieves this by leveraging the transparency (alpha) channel, a well-known concept in computer graphics, to encode bits into pixel translucency changes without modifying pixel color (RGB) values. We demonstrated HiLight’s feasibility using smartphones. By offering an unobtrusive, flexible, and lightweight communication channel between screens and cameras, HiLight allows new HCI and context-aware applications to emerge.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/vlcs14-hilight/index.html" class="no-external">
<div class="listing-date">
Jul 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="39" data-listing-date-sort="1401595200000" data-listing-file-modified-sort="1748580109713" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom14-angora/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobicom14-angora/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom14-angora/index.html" class="no-external">Cutting the Cord: a Robust Wireless Facilities Network for Data Centers</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom14-angora/index.html" class="no-external">
<p>Today’s network control and management traffic are limited by their reliance on existing data networks. Fate sharing in this context is highly undesirable, since control traffic has very different availability and traffic delivery requirements. In this paper, we explore the feasibility of building a dedicated wireless facilities network for data centers. We propose Angora, a low-latency facilities network using low-cost, 60GHz beamforming radios that provides robust paths decoupled from the wired network, and flexibility to adapt to workloads and network dynamics. We describe our solutions to address challenges in link coordination, link interference and network failures. Our testbed measurements and simulation results show that Angora enables large number of low-latency control paths to run concurrently, while providing low latency end-to-end message delivery with high tolerance for radio and rack failures.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom14-angora/index.html" class="no-external">
<div class="listing-date">
Jun 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="40" data-listing-date-sort="1398916800000" data-listing-file-modified-sort="1748580109732" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/wpa14/index.html" class="no-external">

<img loading="lazy" data-src="./publication/wpa14/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/wpa14/index.html" class="no-external">My Smartphone Knows I am Hungry</a>
</h3>
<div class="delink listing-description"><a href="./publication/wpa14/index.html" class="no-external">
<p>Can a smartphone learn our eating habits without the user being in the loop? Clearly, the phone could use checkins based on location to infer that if you were in a cafe, for example, there is a good possibility you might eat or drink something. In this paper, we use inferred behavioral data and location history to predict if you are going to eat or not in the near future. These predictors could serve as a basis for future eating trackers that work unobtrusively in the background of your phone rather than relying on burdensome user input. In this paper, we report on a simple model that predicts the food purchases of a group of undergraduate college students (N=25) using inferred behavioral and location data from smartphones. The 10-week study uses the dining related purchase records from student college cards as ground-truth to validate our prediction model. Initial results show that we can predict food and drink purchases with an accuracy of 74% using three weeks of training data.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/wpa14/index.html" class="no-external">
<div class="listing-date">
May 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="41" data-listing-date-sort="1395374400000" data-listing-file-modified-sort="1748580109621" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ton15-cg/index.html" class="no-external">

<img loading="lazy" data-src="./publication/ton15-cg/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ton15-cg/index.html" class="no-external">Practical Conflict Graphs in the Wild</a>
</h3>
<div class="delink listing-description"><a href="./publication/ton15-cg/index.html" class="no-external">
<p>Today, most spectrum allocation algorithms use conflict graphs to capture interference conditions. The use of conflict graphs, however, is often questioned by the wireless community for two reasons. First, building accurate conflict graphs requires significant overhead, and hence does not scale to outdoor networks. Second, conflict graphs cannot properly capture accumulative interference. In this paper, we use large-scale measurement data as ground truth to understand how severe these problems are and whether they can be overcome. We build “practical” conflict graphs using measurement-calibrated propagation models, which remove the need for exhaustive signal measurements by interpolating signal strengths using calibrated models. Calibrated models are imperfect, and we study the impact of their errors on multiple steps in the process, from calibrating propagation models, predicting signal strengths, to building conflict graphs. At each step, we analyze the introduction, propagation, and final impact of errors by comparing each intermediate result to its ground-truth counterpart. Our work produces several findings. Calibrated propagation models generate location-dependent prediction errors, ultimately producing conservative conflict graphs. While these “estimated conflict graphs” lower spectrum utilization, their conservative nature improves reliability by reducing the impact of accumulative interference. Finally, we propose a graph augmentation technique to address remaining accumulative interference.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ton15-cg/index.html" class="no-external">
<div class="listing-date">
Mar 21, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="42" data-listing-date-sort="1393650000000" data-listing-file-modified-sort="1748580109777" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sigmetrics13_cg/index.html" class="no-external">

<img loading="lazy" data-src="./publication/sigmetrics13_cg/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sigmetrics13_cg/index.html" class="no-external">Practical Conflict Graphs for Dynamic Spectrum Distribution</a>
</h3>
<div class="delink listing-description"><a href="./publication/sigmetrics13_cg/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/sigmetrics13_cg/index.html" class="no-external">
<div class="listing-date">
Mar 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="43" data-listing-date-sort="1391230800000" data-listing-file-modified-sort="1748580109719" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sigcomm12_beam3d/index.html" class="no-external">

<img loading="lazy" data-src="./publication/sigcomm12_beam3d/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sigcomm12_beam3d/index.html" class="no-external">Mirror Mirror on the Ceiling: Flexible Wireless Links for Data Centers</a>
</h3>
<div class="delink listing-description"><a href="./publication/sigcomm12_beam3d/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/sigcomm12_beam3d/index.html" class="no-external">
<div class="listing-date">
Feb 1, 2014
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="44" data-listing-date-sort="1320120000000" data-listing-file-modified-sort="1748580109670" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/hotnet11_beam3d/index.html" class="no-external">

<img loading="lazy" data-src="./publication/hotnet11_beam3d/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotnet11_beam3d/index.html" class="no-external">3D Beamforming for Wireless Data Centers</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotnet11_beam3d/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/hotnet11_beam3d/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2011
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="45" data-listing-date-sort="1314849600000" data-listing-file-modified-sort="1748580109630" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom11_borealis/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobicom11_borealis/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom11_borealis/index.html" class="no-external">I Am the Antenna: Accurate Outdoor AP Location Using Smartphones</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom11_borealis/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom11_borealis/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2011
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="46" data-listing-date-sort="1306900800000" data-listing-file-modified-sort="1748580109729" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sigmetrics11_baron/index.html" class="no-external">

<img loading="lazy" data-src="./publication/sigmetrics11_baron/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sigmetrics11_baron/index.html" class="no-external">On the Stability and Optimality of Universal Swarms</a>
</h3>
<div class="delink listing-description"><a href="./publication/sigmetrics11_baron/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/sigmetrics11_baron/index.html" class="no-external">
<div class="listing-date">
Jun 1, 2011
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="47" data-listing-date-sort="1296536400000" data-listing-file-modified-sort="1748580109635" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/info11_topaz/index.html" class="no-external">

<img loading="lazy" data-src="./publication/info11_topaz/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/info11_topaz/index.html" class="no-external">To Preempt or Not: Tackling Bid and Time-based Cheating in Online Spectrum Auctions</a>
</h3>
<div class="delink listing-description"><a href="./publication/info11_topaz/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/info11_topaz/index.html" class="no-external">
<div class="listing-date">
Feb 1, 2011
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="48" data-listing-date-sort="1291179600000" data-listing-file-modified-sort="1748580109721" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobihoc10_athena/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobihoc10_athena/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobihoc10_athena/index.html" class="no-external">Breaking Bidder Collusion in Large-Scale Spectrum Auctions</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobihoc10_athena/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/mobihoc10_athena/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2010
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="49" data-listing-date-sort="1265000400000" data-listing-file-modified-sort="1748580109707" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/dyspan10-optimus/index.html" class="no-external">

<img loading="lazy" data-src="./publication/dyspan10-optimus/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/dyspan10-optimus/index.html" class="no-external">Optimus: SINR-driven Spectrum Distribution via Constraint Transformation</a>
</h3>
<div class="delink listing-description"><a href="./publication/dyspan10-optimus/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/dyspan10-optimus/index.html" class="no-external">
<div class="listing-date">
Feb 1, 2010
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="50" data-listing-date-sort="1259643600000" data-listing-file-modified-sort="1748580109644" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/crtextbook10/index.html" class="no-external">

<img loading="lazy" data-src="./publication/crtextbook10/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/crtextbook10/index.html" class="no-external">Auction-based Spectrum Markets in Cognitive Radio Networks</a>
</h3>
<div class="delink listing-description"><a href="./publication/crtextbook10/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/crtextbook10/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2009
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="51" data-listing-date-sort="1249099200000" data-listing-file-modified-sort="1748580109727" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/info09_trust/index.html" class="no-external">

<img loading="lazy" data-src="./publication/info09_trust/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/info09_trust/index.html" class="no-external">TRUST: A General Framework for Truthful Double Spectrum Auctions</a>
</h3>
<div class="delink listing-description"><a href="./publication/info09_trust/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/info09_trust/index.html" class="no-external">
<div class="listing-date">
Aug 1, 2009
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="52" data-listing-date-sort="1228107600000" data-listing-file-modified-sort="1748580109753" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobicom08_veritas/index.html" class="no-external">

<img loading="lazy" data-src="./publication/mobicom08_veritas/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobicom08_veritas/index.html" class="no-external">eBay in the Sky: Strategy-Proof Wireless Spectrum Auctions</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobicom08_veritas/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/mobicom08_veritas/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2008
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="53" data-listing-date-sort="1222833600000" data-listing-file-modified-sort="1748580109762" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sdr08/index.html" class="no-external">

<img loading="lazy" data-src="./publication/sdr08/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sdr08/index.html" class="no-external">Traffic-Driven Dynamic Spectrum Auctions</a>
</h3>
<div class="delink listing-description"><a href="./publication/sdr08/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/sdr08/index.html" class="no-external">
<div class="listing-date">
Oct 1, 2008
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="54" data-listing-date-sort="1196485200000" data-listing-file-modified-sort="1748580109628" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/icc07-cora/index.html" class="no-external">

<img loading="lazy" data-src="./publication/icc07-cora/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/icc07-cora/index.html" class="no-external">Correlation based Rate Adaptation via Insights from Incomplete Observations in 802.11 Networks</a>
</h3>
<div class="delink listing-description"><a href="./publication/icc07-cora/index.html" class="no-external">

</a></div>
</div>
<div class="metadata">
<a href="./publication/icc07-cora/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2007
</div>
</a>
</div>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
<nav id="listing-pagination" class="listing-pagination" aria-label="Page Navigation">
  <ul class="pagination"></ul>
</nav>
</div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>