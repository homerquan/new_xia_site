<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Xia Zhou</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-listing/list.min.js"></script>
<script src="site_libs/quarto-listing/quarto-listing.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d3f433bb8225445b39e21615b14794d2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-date','listing-title','listing-author','listing-image','listing-description',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 25,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-date","listing-title","listing-author","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Xia Zhou</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content column-page" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<div class="columns">
<div class="column" style="width:30%;">
<div style="text-align: center">
<p><img src="images/xia.jpg" class="profile-img img-fluid" width="220"></p>
</div>
<div style="text-align: center">
<p><strong>Xia Zhou</strong><br>
Associate Professor<br>
<a href="https://www.cs.columbia.edu">Department of Computer Science, Columbia University</a></p>
</div>
</div><div class="column" style="width:70%;">
<p>I am an Associate Professor in the <a href="https://www.cs.columbia.edu">Department of Computer Science</a> at Columbia University. I direct the <a href="https://mobilex.cs.columbia.edu">Mobile X Lab</a>. Before joining Columbia, I was the co-director of the <a href="https://dartnets.cs.dartmouth.edu">Dartmouth Networking and Ubiquitous Systems (DartNets) Lab</a> and the <a href="https://rlab.cs.dartmouth.edu">Dartmouth Reality and Robotics Lab (RLab)</a> at Dartmouth College. I was also affiliated with the <a href="https://ah.cs.dartmouth.edu">Augmented Health Lab</a>. I received my PhD in Computer Science at <a href="https://www.cs.ucsb.edu">UC Santa Barbara</a> in June 2013, working under the supervision of <a href="https://users.soe.ucsc.edu/~zheng/">Prof.&nbsp;Heather Zheng</a>. I was a visiting faculty in <a href="https://www.ntu.edu.tw/english/">National Taiwan University</a> from December 2016 to February 2017, and in <a href="https://www.cam.ac.uk">University of Cambridge</a> from April 2017 to June 2017.</p>
<p>My research interest lies broadly in mobile computing and its intersection with other disciplines. Most of my current projects center on light—a ubiquitous medium around us. We explore a range of projects that turn light into a powerful medium for data communication and object/behavioral sensing. Check out our <a href="https://mobilex.cs.columbia.edu/demos/">demo videos</a> to learn more details. My PhD work designed network systems to handle the massive volume and unpredictable nature of today’s data traffic (see more in our <a href="htt">MobiCom’08</a></p>
</div><section id="top-publications" class="level2">
<h2 class="anchored" data-anchor-id="top-publications">Top Publications</h2>
<div id="listing-listing" class="quarto-listing quarto-listing-container-default">
<div class="list quarto-listing-default">
<div class="quarto-post image-right" data-index="0" data-listing-file-modified-sort="1748580109639" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/chi20-threadsense/index.html" class="no-external">

<img loading="lazy" src="./publication/chi20-threadsense/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/chi20-threadsense/index.html" class="no-external">ThreadSense: Locating Touch on an Extremely Thin Interactive Thread</a>
</h3>
<div class="delink listing-description"><a href="./publication/chi20-threadsense/index.html" class="no-external">
<p>We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/chi20-threadsense/index.html" class="no-external">
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="1" data-listing-file-modified-sort="1748580109738" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/hotmobile20-polartag/index.html" class="no-external">

<img loading="lazy" src="./publication/hotmobile20-polartag/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/hotmobile20-polartag/index.html" class="no-external">PolarTag: Invisible Data with Light Polarization</a>
</h3>
<div class="delink listing-description"><a href="./publication/hotmobile20-polartag/index.html" class="no-external">
<p>Visual tags (e.g.,barcodes, QR codes) are ubiquitous in modern day life, though they rely on obtrusive geometric patterns to encode data, degrading the overall user experience. We propose a new paradigm of passive visual tags which utilizes light polarization to imperceptibly encode data using cheap, widely-available components. The tag and its data can be extracted from background scenery using off-the-shelf cameras with inexpensive LCD shutters attached atop camera lenses. We examine the feasibility of this design with real-world experiments. Initial results show zero bit errors at distances up to 3.0~m, an angular-detection range of 110’’, and robustness to manifold ambient light and occlusion scenarios.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/hotmobile20-polartag/index.html" class="no-external">
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="2" data-listing-date-sort="1658894400000" data-listing-file-modified-sort="1748580109680" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/cogsci22-sl/index.html" class="no-external">

<img loading="lazy" src="./publication/cogsci22-sl/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/cogsci22-sl/index.html" class="no-external">Overlapping Semantic Representations of Sign and Speech in Novice Sign Language Learners</a>
</h3>
<div class="delink listing-description"><a href="./publication/cogsci22-sl/index.html" class="no-external">
<p>The presence of semantic information in multivariate patterns of neural activity has been explored as a method of measuring knowledge and learning. Using fMRI, we investigated whether novice learners of American Sign Language (ASL) showed overlapping representations of semantic categories for words presented in a well-known (English) or newly learned (ASL) language. We find evidence of neural patterns that were partially shared between sign and speech in novice participants. This result provides evidence for the influence of even brief learning on neural representations in cross-modality language processing.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/cogsci22-sl/index.html" class="no-external">
<div class="listing-date">
Jul 27, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="3" data-listing-date-sort="1656388800000" data-listing-file-modified-sort="1748580109659" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/mobisys22-sunflower/index.html" class="no-external">

<img loading="lazy" src="./publication/mobisys22-sunflower/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/mobisys22-sunflower/index.html" class="no-external">Sunflower: Locating Underwater Robots From the Air</a>
</h3>
<div class="delink listing-description"><a href="./publication/mobisys22-sunflower/index.html" class="no-external">
<p>Locating underwater robots is fundamental for enabling important underwater applications. The current mainstream method requires a physical infrastructure with relays on the water surface, which is largely ad-hoc, introduces a significant logistical overhead, and entails limited scalability. Our work, Sunflower, presents the first demonstration of wireless, 3D localization across the air-water interface – eliminating the need for additional infrastructure on the water surface. Specifically, we propose a laser-based sensing system to enable aerial drones to directly locate underwater robots. The Sunflower system consists of a queen and a worker component on a drone and each tracked underwater robot, respectively. To achieve robust sensing, key system elements include (1) a pinhole- based sensing mechanism to address the sensing skew at air-water boundary and determine the incident angle on the worker, (2) a novel optical-fiber sensing ring to sense weak retroreflected light, (3) a laser-optimized backscatter communication design that exploits laser polarization to maximize retroreflected energy, and (4) the necessary models and algorithms for underwater sensing. Real- world experiments demonstrate that our Sunflower system achieves average localization error of 9.7 cm with ranges up to 3.8 m and is robust against ambient light interference and wave conditions.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/mobisys22-sunflower/index.html" class="no-external">
<div class="listing-date">
Jun 28, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="4" data-listing-date-sort="1641358800000" data-listing-file-modified-sort="1748580109655" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/comsnets22-light/index.html" class="no-external">

<img loading="lazy" src="./publication/comsnets22-light/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/comsnets22-light/index.html" class="no-external">Air-Water Communication and Sensing with Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/comsnets22-light/index.html" class="no-external">
<p>The ability to communicate and sense across the air- water boundary is essential for efficient exploration and monitoring of the underwater world. Existing wireless solutions for communication and sensing typically focus on a single physical medium and fall short in achieving high-bandwidth communication and accurate sensing across the air-water interface without any relays on the water surface. We study the use of laser light in this context given its ability to effectively pass the air-water boundary. We present a holistic system framework to address practical challenges such as ambient light interference and environmental dynamics. The proposed AmphiLight framework achieves 5 Mbps bi-directional throughput and zero bit error rate with ranges up to 6.1 m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics. Our ongoing effort extends to realizing robust air-water sensing that enables aerial drones to track multiple underwater robots for topology planning and coordination.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/comsnets22-light/index.html" class="no-external">
<div class="listing-date">
Jan 5, 2022
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="5" data-listing-date-sort="1630468800000" data-listing-file-modified-sort="1748580109648" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp21-facesense/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp21-facesense/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp21-facesense/index.html" class="no-external">FaceSense: Sensing Face Touch with an Ear-worn System</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp21-facesense/index.html" class="no-external">
<p>Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth)increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSenseintegrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch.Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp21-facesense/index.html" class="no-external">
<div class="listing-date">
Sep 1, 2021
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="6" data-listing-date-sort="1606798800000" data-listing-file-modified-sort="1748580109711" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/ubicomp20-motion/index.html" class="no-external">

<img loading="lazy" src="./publication/ubicomp20-motion/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/ubicomp20-motion/index.html" class="no-external">Teaching American Sign Language in Mixed Reality</a>
</h3>
<div class="delink listing-description"><a href="./publication/ubicomp20-motion/index.html" class="no-external">
<p>This paper presents a holistic system to scale up the teaching and learning of vocabulary words of American Sign Language (ASL). The system leverages the most recent mixed-reality technology to allow the user to perceive her own hands in an immersive learning environment with first- and third-person views for motion demonstration and practice. Precise motion sensing is used to record and evaluate motion, providing real-time feedback tailored to the specific learner. As part of this evaluation, learner motions are matched to features derived from the Hamburg Notation System (HNS) developed by sign-language linguists. We develop a prototype to evaluate the efficacy of mixed-reality-based interactive motion teaching. Results with 60 participants show a statistically significant improvement in learning ASL signs when using our system, in comparison to traditional desktop-based, non-interactive learning. We expect this approach to ultimately allow teaching and guided practice of thousands of signs.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/ubicomp20-motion/index.html" class="no-external">
<div class="listing-date">
Dec 1, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="7" data-listing-date-sort="1605502800000" data-listing-file-modified-sort="1748580109769" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/sensys20-glucose/index.html" class="no-external">

<img loading="lazy" src="./publication/sensys20-glucose/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/sensys20-glucose/index.html" class="no-external">Noninvasive Glucose Monitoring Using Polarized Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/sensys20-glucose/index.html" class="no-external">
<p>We propose a compact noninvasive glucose monitoring system using polarized light, where a user simply needs to place her palm on the device for measuring her current glucose concentration level. The primary innovation of our system is the ability to minimize light scattering from the skin and extract weak changes in light polarization to estimate glucose concentration, all using low-cost hardware. Our system exploits multiple wavelengths and light intensity levels to mitigate the effect of user diversity and confounding factors (e.g., collagen and elastin in the dermis). It then infers glucose concentration using a generic learning model, thus no additional calibration is needed. We design and fabricate a compact (17 cm × 10 cm × 5 cm) and low-cost (i.e., &lt;$250) prototype using off-the-shelf hardware. We evaluate our system with 41 diabetic patients and 9 healthy participants. In comparison to a continuous glucose monitor approved by U.S. Food and Drug Administration (FDA), 89% of our results are within zone A (clinically accurate) of the Clarke Error Grid. The absolute relative difference (ARD) is 10% . The r and p values of the Pearson correlation coefficients between our predicted glucose concentration and reference glucose concentration are 0.91 and 1.6 × 10−143, respectively. These errors are comparable with FDA-approved glucose sensors, which achieve ≈90% clinical accuracy with a 10% mean ARD.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/sensys20-glucose/index.html" class="no-external">
<div class="listing-date">
Nov 16, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="8" data-listing-date-sort="1604203200000" data-listing-file-modified-sort="1748580109751" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="11">
<div class="thumbnail"><a href="./publication/nsdi20-amphilight/index.html" class="no-external">

<img loading="lazy" src="./publication/nsdi20-amphilight/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/nsdi20-amphilight/index.html" class="no-external">AmphiLight: Direct Air-Water Communication with Laser Light</a>
</h3>
<div class="delink listing-description"><a href="./publication/nsdi20-amphilight/index.html" class="no-external">
<p>Air-water communication is fundamental for efficient underwater operations, such as environmental monitoring, surveying, or coordinating of heterogeneous aerial and underwater systems. Existing wireless techniques mostly focus on a single physical medium and fall short in achieving high-bandwidth bidirectional communication across the air-water interface. We propose a bidirectional, direct air-water wireless communication link based on laser light, capable of (1) adapting to water dynamics with ultrasonic sensing and (2) steering within a full 3D hemisphere using only a MEMS mirror and passive optical elements. In real-world experiments, our system achieves static throughputs up to 5.04 Mbps, zero-BER transmission ranges up to 6.1m in strong ambient light conditions, and connection time improvements between 47.1% and 29.5% during wave dynamics.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/nsdi20-amphilight/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2020
</div>
</a>
</div>
</div>
<div class="quarto-post image-right" data-index="9" data-listing-date-sort="1572580800000" data-listing-file-modified-sort="1748580109702" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="1">
<div class="thumbnail"><a href="./publication/LongitudinalAnalysis/index.html" class="no-external">

<img loading="lazy" src="./publication/LongitudinalAnalysis/featured.jpg" class="thumbnail-image" alt="3D Wi-Fi reflector brain map">

</a></div>
<div class="body">
<h3 class="no-anchor listing-title">
<a href="./publication/LongitudinalAnalysis/index.html" class="no-external">Longitudinal Analysis of a Campus Wi-Fi Network</a>
</h3>
<div class="delink listing-description"><a href="./publication/LongitudinalAnalysis/index.html" class="no-external">
<p>In this paper we describe and characterize the largest Wi-Fi network trace ever published: spanning seven years, approximately 3000 distinct access points, 40,000 authenticated users, and 600,000 distinct Wi-Fi stations. The 7TB of raw data are pre-processed into connection sessions, which are made available for the research community. We describe the methods used to capture and process the traces, and characterize the most prominent trends and changes during the seven-year span of the trace. Furthermore, this Wi-Fi network covers the campus of Dartmouth College, the same campus detailed a decade earlier in seminal papers about that network and its users’ network behavior. We thus are able to comment on changes in patterns of usage, connection, and mobility in Wi-Fi deployments.</p>
</a></div>
</div>
<div class="metadata">
<a href="./publication/LongitudinalAnalysis/index.html" class="no-external">
<div class="listing-date">
Nov 1, 2019
</div>
</a>
</div>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
</div>



</section>
</div>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>